import os
import datetime
import requests
import numpy as np
import pandas as pd

from bs4 import BeautifulSoup
from urllib.parse import urlparse
from urllib.request import urlopen
from urllib.error import URLError


# 작업 시간 시작
print("작업 시간 {} 시 시작".format(datetime.datetime.now()))

# 데이터 경로 및 추출
path = os.path.sep.join(["test.csv"])
malware_data = pd.read_csv(path)
def html_content(url):
    req = requests.get(url).content
    print(req)

for i in malware_data["url"][1:2]:
    html_content(i)

"""
def PortCheck(url):
    # status_code 상태 코드 추출
    request_successfully = 0
    request_fail = 0
    try:
        malware_port_check = requests.get(url, timeout=1).status_code
        print(url, malware_port_check)
        if malware_port_check:
            request_successfully += 1
    except requests.exceptions.ReadTimeout as RT:
        if RT:
            request_fail += 1
    except requests.exceptions.ConnectionError as CE:
        if CE:
            request_fail += 1
    return request_successfully, request_fail

def Iframe(url):
    try:
        req = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, verify=False, timeout=10).content
        soup = BeautifulSoup(req, "html.parser")
        for i_frame in soup.find_all('i_frame', width=True, height=True, frameBorder=True):
            if i_frame['width'] == "0" and i_frame['height'] == "0" and i_frame['frameBorder'] == "0":
                return -1
            if i_frame['width'] == "0" or i_frame['height'] == "0" or i_frame['frameBorder'] == "0":
                return 0
        return 1
    except requests.exceptions.ConnectionError:
        return -1
    except requests.exceptions.ReadTimeout:
        return -1
    except requests.exceptions.TooManyRedirects:
        return -1
    except URLError:
        return -1
    except requests.exceptions.InvalidSchema:
        return -1
    except requests.exceptions.InvalidURL:
        return -1

def sfh(url):
    try:
        resp = requests.get(url, headers={"User-Agent": "Mozilla/5.0"}, verify=False, timeout=10).content
        soup = BeautifulSoup(resp, 'html.parser')
        for form in soup.find_all('form', action=True):
            if form['action'] == "" or form['action'] == "about:blank":
                return -1
            elif url not in form['action'] and url not in form['action']:
                return 0
            else:
                return 1
        return 1
    except requests.exceptions.ConnectionError:
        return -1
    except requests.exceptions.ReadTimeout:
        return -1
    except requests.exceptions.TooManyRedirects:
        return -1
    except URLError:
        return -1
    except requests.exceptions.InvalidSchema:
        return -1

malware_data["Iframe"] = malware_data["url"].apply(lambda i: Iframe(i))
malware_data["SFH"] = malware_data["url"].apply(lambda i: sfh(i))
malware_data.to_csv("test.csv", index=False)
"""